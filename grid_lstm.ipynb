{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuFaDyIQEXvW",
        "outputId": "b4bc39f6-eabe-458d-d9fb-ba5b68fad9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# os.chdir('/content/drive/My Drive/metall/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCeNitkMS8kS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Прогноз металлические заготовки/'\n",
        "file_name = 'data_for_lstm.csv'\n",
        "\n",
        "df = pd.read_csv(path + file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8mOjEYp2dn5"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H0wyuTC2b6x"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "def lstm_trained(features_set,labels,epochs = 40, batch_size = 4,num_layers=1,p=0.0,units=48,optimizer='adam'):\n",
        "  '''\n",
        "  \n",
        "  return trained lstm\n",
        "\n",
        "  '''\n",
        "  model = Sequential()\n",
        "  if num_layers==2:\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(features_set.shape[1],features_set.shape[2] )))\n",
        "    model.add(Dropout(p))\n",
        "    \n",
        "  elif num_layers>2:\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(features_set.shape[1],features_set.shape[2] )))\n",
        "    for layer in range(num_layers-1):\n",
        "      model.add(LSTM(units=units, return_sequences=True))\n",
        "      model.add(Dropout(p))\n",
        "\n",
        "  model.add(LSTM(units=50))\n",
        "  model.add(Dropout(p))\n",
        "  model.add(Dense(units = 1))\n",
        "  model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
        "  model.fit(features_set, labels, epochs = epochs, batch_size = batch_size, verbose=1)\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQW8I5CZ-RqQ"
      },
      "source": [
        "## lstm score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O87VMflNwJ0g"
      },
      "outputs": [],
      "source": [
        "def lstm_predict(df_train,df_test,shift, epochs = 2, batch_size = 8,num_layers=1,p=0.0,units=48,optimizer='adam'):\n",
        "  '''\n",
        "\n",
        "  returns the prediction lstm\n",
        "\n",
        "  params:\n",
        "  shift - the length of the background\n",
        "  other - dataframes or params for lstm\n",
        "\n",
        "  '''\n",
        "  scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "  df_training_processed = df_train.iloc[:, :].values\n",
        "  df_training_scaled = scaler.fit_transform(df_training_processed)\n",
        "\n",
        "  features_set = []\n",
        "  labels = []\n",
        "  # We will make predictions for the next week based on the history (length of shift=length_history)\n",
        "  # Let's form this selection\n",
        "  len_ = df_training_scaled.shape[0]\n",
        "  for i in range(shift, len_):\n",
        "      features_set.append(df_training_scaled[i-shift:i, :])\n",
        "      labels.append(df_training_scaled[i, 0])\n",
        "\n",
        "  # Convert to an np array and make a suitable format for LSTM, train the model\n",
        "  features_set, labels = np.array(features_set), np.array(labels)\n",
        "  features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], features_set.shape[2]))\n",
        "  model = lstm_trained(features_set, labels,epochs=epochs,batch_size=batch_size, p=p, num_layers=num_layers, optimizer=optimizer, units=units)\n",
        "\n",
        "  # Let's create a sample for testing\n",
        "  df_total = pd.concat((df_train, df_test), axis=0)\n",
        "  test_inputs = df_total[len(df_total) - len(df_test) - shift:]\n",
        "  test_inputs_processed = test_inputs.iloc[:, :].values\n",
        "  test_inputs_scaled= scaler.transform(test_inputs_processed)\n",
        "  test_features = []\n",
        "  for i in range(shift, len(test_inputs_scaled)):\n",
        "      test_features.append(test_inputs_scaled[i-shift:i, :])\n",
        "\n",
        "  # Convert again to an np array and make a format suitable for LSTM  test_features = np.array(test_features)\n",
        "  test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], test_features.shape[2]))\n",
        "\n",
        "  predictions = model.predict(test_features)\n",
        "  predictions=predictions.reshape(df_test.shape[0],)\n",
        "\n",
        "  test_data_processed = (df_test.copy()).iloc[:].values\n",
        "  test_data_processed[:,0] = predictions\n",
        "  test_data_processed = scaler.inverse_transform(test_data_processed)\n",
        "  predictions = test_data_processed[:,0]\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o25vjJMj7g7i"
      },
      "source": [
        "## Grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgxpOxfgfQZ-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "grid = ParameterGrid({'num_layers': [0,1], \n",
        "                      'epochs':[40,60],\n",
        "                      'p':[0.0,0.05,0.1],\n",
        "                      'length_history':[4,8],\n",
        "                      'n_splits':[10,20],\n",
        "                      'units':[8,12,24,48],\n",
        "                      'min_len_datasets':[30],\n",
        "                      'length_prediction':[1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_-QiT4n-OqY"
      },
      "outputs": [],
      "source": [
        "#grid = ParameterGrid({'num_layers': [1], \n",
        "#                      'epochs':[40],\n",
        "#                      'p':[0.2],\n",
        "#                      'length_history':[8],\n",
        "#                      'n_splits':[10],\n",
        "#                      'min_len_datasets':[30],\n",
        "#                      'length_prediction':[1]})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's calculate the average mae for different grid architectures using timeseriescv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MApH5HL69ShI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FJ1IPfIyf5cM"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "scores = dict()\n",
        "\n",
        "for item in tqdm(grid):  \n",
        "  values = (df.copy()).values\n",
        "  length_prediction = item['length_prediction']\n",
        "  n_splits = item['n_splits']\n",
        "  min_len_datasets= item['min_len_datasets']\n",
        "  length_history=item['length_history']\n",
        "  tscv = TimeSeriesSplit(n_splits=n_splits,test_size=length_prediction)\n",
        "  res = []\n",
        "  for id_train,id_test in tscv.split(values):\n",
        "    df_train=df.iloc[id_train,:].copy()\n",
        "    if (len(df_train)<length_history+min_len_datasets):\n",
        "      continue\n",
        "    df_test=df.iloc[id_test,:].copy()\n",
        "    predictions = lstm_predict(df_train.copy(), df_test.copy(), shift=length_history,\n",
        "                               epochs=item['epochs'],p=item['p'],num_layers=item['num_layers'],units=item['units'])\n",
        "    error = 0\n",
        "    targets = df_test['target'].values\n",
        "    for i in range(len(predictions)):\n",
        "      error+=abs(predictions[i]-targets[i])\n",
        "    res.append(error/len(predictions))\n",
        "    print(scores)\n",
        "\n",
        "  scores[str(item)] = [np.mean(res)] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AEupbo-ARhu"
      },
      "outputs": [],
      "source": [
        "scores"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "grid_lstm.ipynb\"\"",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}